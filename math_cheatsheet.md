# ✅ NLP LLM Builder’s Math Cheat Sheet

## 📌 What to Know Deeply
- Tokenization: split text into tokens or subwords.
- Embedding: map tokens to dense vectors.
- Vector similarity: cosine similarity, dot product.
- Matrix shapes: input, embedding, output.
- Logistic Regression: sigmoid, binary prediction.
- Gradient Descent: adjusts weights to minimize loss.
- Attention: words attend to other words (big idea).

## 📌 Practical Math
- Probability basics: log-likelihood.
- Cross-Entropy Loss: why it penalizes wrong predictions.
- Matrix multiply shapes.
- Backpropagation: adjust by gradients.
- Activation functions: sigmoid, softmax.

## 📌 Just Look Up When Needed
- Optimizers: Adam, SGD.
- Transformer equations: scaled dot-product.
- Embedding training math: skip-gram.
- Vector DB indexing: cosine similarity.

## ✅ Key takeaway
Understand what each part does. Libraries do the math. Shapes & flow matter most.

---

# ✅ NLP Pipeline Diagram

```
📦 Dataset (e.g., Movie Reviews)
   ↓
🧹 Pre-processing → Tokenize → Clean → Remove Stopwords
   ↓
🔢 Feature Extraction → Bag-of-Words, Frequencies, Embedding Lookup
   ↓
📐 Logistic Regression → Sigmoid → Cost Function
   ↓
⚙️ Gradient Descent → Update Weights
   ↓
✅ Predictions → Probabilities → Class Label (0/1)
   ↓
📊 Evaluate Accuracy → Report Metrics
```

**This is the full loop: raw text → features → model → prediction → result.**
