{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ecaef91",
   "metadata": {},
   "source": [
    "### Step 1️⃣ — Setup & Import\n",
    "\n",
    "#### What you’ll do:\n",
    "\n",
    "- Import libraries.\n",
    "- Download corpus if needed.\n",
    "- Inspect the data.\n",
    "\n",
    "#### Why:\n",
    "You must understand the raw data to design how to process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323d01c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import movie_reviews\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Download corpus if not done yet\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "# Inspect data\n",
    "print(\"Total files:\", len(movie_reviews.fileids()))\n",
    "print(\"Sample file IDs:\", movie_reviews.fileids()[:5])\n",
    "print(\"Categories:\", movie_reviews.categories())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32262743",
   "metadata": {},
   "source": [
    "### Step 2️⃣ — Split Data\n",
    "\n",
    "#### What you’ll do:\n",
    "\n",
    "- Make training & testing sets.\n",
    "- Shuffle to avoid order bias.\n",
    "\n",
    "#### Why:\n",
    "Good ML practice: always separate training & validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99468031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pos & neg file IDs\n",
    "pos_files = movie_reviews.fileids('pos')\n",
    "neg_files = movie_reviews.fileids('neg')\n",
    "\n",
    "# Shuffle for randomness\n",
    "random.shuffle(pos_files)\n",
    "random.shuffle(neg_files)\n",
    "\n",
    "# Split: 80% train, 20% test\n",
    "train_pos = pos_files[:800]\n",
    "test_pos = pos_files[800:]\n",
    "\n",
    "train_neg = neg_files[:800]\n",
    "test_neg = neg_files[800:]\n",
    "\n",
    "print(f\"Training samples: {len(train_pos) + len(train_neg)}\")\n",
    "print(f\"Testing samples: {len(test_pos) + len(test_neg)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14139de2",
   "metadata": {},
   "source": [
    "### Step 3️⃣ — Write a Text Preprocessor\n",
    "\n",
    "#### What you’ll do:\n",
    "\n",
    "- Tokenize words.\n",
    "- Lowercase.\n",
    "- Remove punctuation.\n",
    "- Remove stopwords.\n",
    "\n",
    "#### Why:\n",
    "Cleaner data = better features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d91b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def process_review(words):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Keep only alphabetic words, lowercase, no stopwords\n",
    "    cleaned = [w.lower() for w in words if w.isalpha() and w.lower() not in stop_words]\n",
    "    return cleaned\n",
    "\n",
    "# Test\n",
    "sample_words = movie_reviews.words(train_pos[0])\n",
    "print(process_review(sample_words)[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2562aa73",
   "metadata": {},
   "source": [
    "### Step 4️⃣ — Build Word Frequency Dictionary\n",
    "\n",
    "#### What you’ll do:\n",
    "\n",
    "- For all training data, count (word, label) pairs.\n",
    "\n",
    "#### Why:\n",
    "You’ll use these counts for your features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8105b472",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = defaultdict(int)\n",
    "\n",
    "# Process positive reviews\n",
    "for fileid in train_pos:\n",
    "    words = process_review(movie_reviews.words(fileid))\n",
    "    for word in words:\n",
    "        freqs[(word, 1.0)] += 1\n",
    "\n",
    "# Process negative reviews\n",
    "for fileid in train_neg:\n",
    "    words = process_review(movie_reviews.words(fileid))\n",
    "    for word in words:\n",
    "        freqs[(word, 0.0)] += 1\n",
    "\n",
    "print(list(freqs.items())[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2761945",
   "metadata": {},
   "source": [
    "### Step 5️⃣ — Create extract_features\n",
    "\n",
    "#### What you’ll do:\n",
    "\n",
    "Given a review, count:\n",
    "\n",
    "- How many words are positive.\n",
    "- How many words are negative.\n",
    "- Always add a bias term.\n",
    "\n",
    "#### Why:\n",
    "Transforms raw text → numeric feature vector [1, pos_count, neg_count]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5039ac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(words, freqs):\n",
    "    x = np.zeros(3)  # bias + pos + neg\n",
    "    x[0] = 1  # bias\n",
    "\n",
    "    for word in words:\n",
    "        x[1] += freqs.get((word, 1.0), 0)\n",
    "        x[2] += freqs.get((word, 0.0), 0)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0f3d36",
   "metadata": {},
   "source": [
    "### Step 6️⃣ — Create Training Matrix\n",
    "\n",
    "#### What you’ll do:\n",
    "\n",
    "- Loop through all training reviews.\n",
    "- For each, get its [1, pos_count, neg_count] feature vector.\n",
    "\n",
    "#### Why:\n",
    "- This is your X.\n",
    "- Labels are your y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83d4a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All train files + labels\n",
    "train_files = train_pos + train_neg\n",
    "train_labels = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "\n",
    "# Matrix X\n",
    "X_train = np.zeros((len(train_files), 3))\n",
    "y_train = train_labels.reshape(-1, 1)\n",
    "\n",
    "for i, fileid in enumerate(train_files):\n",
    "    words = process_review(movie_reviews.words(fileid))\n",
    "    X_train[i, :] = extract_features(words, freqs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d070532b",
   "metadata": {},
   "source": [
    "### Step 7️⃣ — Write Logistic Regression Functions\n",
    "\n",
    "- 1️⃣ Sigmoid\n",
    "- 2️⃣ Cost\n",
    "- 3️⃣ Gradient Descent\n",
    "\n",
    "#### Why:\n",
    "Core math for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302bdb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    m = X.shape[0]\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        z = np.dot(X, theta)\n",
    "        h = sigmoid(z)\n",
    "        J = -(1/m) * (np.dot(y.T, np.log(h)) + np.dot((1-y).T, np.log(1 - h)))\n",
    "        gradient = (1/m) * np.dot(X.T, (h - y))\n",
    "        theta -= alpha * gradient\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iter {i}: Cost {float(J)}\")\n",
    "\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eecbbcd",
   "metadata": {},
   "source": [
    "### Step 8️⃣ — Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076c5494",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros((3, 1))\n",
    "theta = gradient_descent(X_train, y_train, theta, alpha=1e-7, num_iters=1500)\n",
    "\n",
    "print(\"Final theta:\", theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f72c017",
   "metadata": {},
   "source": [
    "### Step 9️⃣ — Evaluate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae4f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "test_files = test_pos + test_neg\n",
    "test_labels = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))\n",
    "X_test = np.zeros((len(test_files), 3))\n",
    "\n",
    "for i, fileid in enumerate(test_files):\n",
    "    words = process_review(movie_reviews.words(fileid))\n",
    "    X_test[i, :] = extract_features(words, freqs)\n",
    "\n",
    "# Predict\n",
    "z = np.dot(X_test, theta)\n",
    "preds = sigmoid(z)\n",
    "pred_labels = preds >= 0.5\n",
    "\n",
    "accuracy = np.mean(pred_labels.flatten() == test_labels)\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
